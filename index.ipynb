{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbf84ad",
   "metadata": {},
   "source": [
    "# Random Forest with Pyspark Introduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b8ee61",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson, you will walk through how to use PySpark for the classification of Iris flowers with a Random Forest Classifier. The dataset is located under the `data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafbcde",
   "metadata": {},
   "source": [
    "## Objectives  \n",
    "\n",
    "* Read a dataset into a PySpark DataFrame\n",
    "* Implement a random forest classifier with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b0b47",
   "metadata": {},
   "source": [
    "> Before continuing, check the version of PySpark installed on the machine. It should be above 3.1.\n",
    "> \n",
    "> You will run this notebook in a `pyspark-env` environment following [these setup instructions without docker](https://github.com/learn-co-curriculum/dsc-spark-docker-installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e47cf66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession  \u001b[38;5;66;03m# entry point for pyspark\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# instantiate spark instance\u001b[39;00m\n\u001b[0;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      5\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Iris\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession  # entry point for pyspark\n",
    "\n",
    "# instantiate spark instance\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Random Forest Iris\").master(\"local[*]\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1602ec",
   "metadata": {},
   "source": [
    "After version 3.0, `SparkSession` is the main entry point for Spark. `SparkSession.builder` creates a spark session. Any thing can go into the `appName()` to specify which jobs you are running currently. Once the spark session is instantiated, if you are running on your local machine, you can access the Spark UI at `localhost:4040` to view jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./data/IRIS.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()  # to see the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f096a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()  # or df.show(Truncate=false) if you'd like to see all the contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf3a26",
   "metadata": {},
   "source": [
    "Check to see what the type is for the DataFrame you have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f722c4",
   "metadata": {},
   "source": [
    "Go ahead and run some exploratory data analysis on the dataset. You can easily turn the PySpark DataFrame into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(df.take(100), columns=df.columns)\n",
    "pandas_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec21d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3a921",
   "metadata": {},
   "source": [
    "Once the exploratory data analysis is done, you can start feature transforming to prepare for feataure engineering. Feature transforming means scaling, modifying features to be used for train/test validation, and converting. For this purpose, you will use the `VectorAssembler` in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff867c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = [\n",
    "    \"sepal_length\",\n",
    "    \"sepal_width\",\n",
    "    \"petal_length\",\n",
    "    \"petal_width\",\n",
    "]  # insert numeric cols\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)  # just use the same dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acecb9c",
   "metadata": {},
   "source": [
    "This should have created another column in your dataframe called `features` as you have denoted in `outputCol`. You can use the `StringIndexer` to encode the string column of species to a label index. By default, the labels are assigned according to the frequencies (for imbalanced dataset). The most frequent species would get an index of 0. For a balanced dataset, whichever string appears first will get 0, then so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labeler = StringIndexer(inputCol=\"species\", outputCol=\"encoded\")\n",
    "df = labeler.fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4b169",
   "metadata": {},
   "source": [
    "The DataFrame now has a new column named `encoded` with new values populated. You can check the new columns have been added to the PySpark DataFrame by creating a new Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ecd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.take(10), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a075d9c",
   "metadata": {},
   "source": [
    "Now you have transformed the data as needed. To begin building your model, you need to split the data into a train/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit(\n",
    "    [0.7, 0.3], seed=42\n",
    ")\n",
    "print(f\"Train dataset count: {str(train.count())}\")\n",
    "print(f\"Test dataset count: {str(test.count())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20771161",
   "metadata": {},
   "source": [
    "Next you will need to instantiate the `RandomForestClassifier` and train the model. At this point before you run the next cell, open up the Spark UI by typing `localhost:4040` into your browser, then navigating to the executors tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e56ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"encoded\")\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c029d",
   "metadata": {},
   "source": [
    "`featuresCol` is the list of features of the dataframe, which means if you have more features you'd like to include, you could put in a list. You create the model by fitting on the training dataset, then validate it by making predictions on the test dataset. `model.transform(test)` will create new columns, like `rawPrediction`, `prediction`, and `probability`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28737f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the columns names here are different, do a `printSchema` on top of predictions to see the correct column names\n",
    "predictions.select(\n",
    "    \"sepal_length\",\n",
    "    \"sepal_width\",\n",
    "    \"petal_length\",\n",
    "    \"petal_width\",\n",
    "    \"encoded\",\n",
    "    \"rawPrediction\",\n",
    "    \"prediction\",\n",
    "    \"probability\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59b194",
   "metadata": {},
   "source": [
    "You have a trained model, go ahead and evaluate the model by using the `MulticlassClassificationEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"encoded\", predictionCol=\"prediction\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}%\")\n",
    "test_error = 1.0 - accuracy\n",
    "print(f\"Test Error = {test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7a8d2",
   "metadata": {},
   "source": [
    "As you can see, the model performs with 97.8% accuracy and has a test error of 0.021. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
